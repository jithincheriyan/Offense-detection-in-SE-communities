{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "The code block below is the UN pipeline that uses the politeness features. That is not included in the present pipeline. In the working pipeline, we use the other set of features (TF-IDF and Sentiment)\n",
   "metadata": {
    "cell_id": "1d346081fc3a4c4a8b1646bdf41990c6",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.78125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1b4ed823342b474eb10a8f8cbdfafd2e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "369662a9",
    "execution_start": 1654509026723,
    "execution_millis": 44,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 585
   },
   "source": "# def get_politeness_features(comment):  # this function accepts the comment and returns a DF of politeness features\n#     r = robjects.r  # this block finds the politeness features of a comment, makes it a DF\n#     r['source']('/work/Politeness_trial.R')  # C:\\\\Users\\\\cheji902\\Dropbox\\\\pycharam works\\\\Info_407_2020\\\\    \n#     politeness_function_r = robjects.globalenv['politeness_function']  # 'politeness_function' is the function defined in R source code\n#     df_polite_r = politeness_function_r(comment)\n#     df_polite = pd.DataFrame(pandas2ri.py2rpy(df_polite_r)).transpose()\n#     return df_polite\n\n# def level_1_UN_prediction(comment):  # this function accepts the comment, gets politeness, sentimental and tf-idf features\n#     df_polite = get_politeness_features(comment)\n\n#     # this block loads the vectrizer, and converts the comment to tf-idf vector, then concatenates with the politeness features\n#     vectorizer = pickle.load(open('/work/L1_ML_Unwelcoming_V1_Vectorizer.pkl', 'rb'))\n#     vect = vectorizer.transform([comment_preprocess(comment)])\n#     vect = pd.DataFrame(vect.toarray(), columns=vectorizer.get_feature_names())\n#     vect = pd.concat([df_polite, vect], axis=1)\n\n#     analyser = SentimentIntensityAnalyzer()  # this block is to get the sentiment score and make it a DF, combine to previous feature vector\n#     score = analyser.polarity_scores(comment)\n#     key = 1 if score[\"compound\"] > 0.5 else 2 if score[\"compound\"] < -0.5 else 0   # sentiments: 1->positive, 0->neutral, 2->negative\n#     senti_df = pd.DataFrame({\"Sentiment\": [score[\"compound\"]],\n#                              \"Sentiment_Score\": [key]})\n#     vect = pd.concat([senti_df, vect], axis=1)\n\n#     SVM = pickle.load(open('/work/L1_SVM_Unwelcoming_V1_Classifier.pkl', 'rb'))\n#     SVM_precict = SVM.predict(vect)\n\n#     print(\"\\n Level 1 UN prediction :\", SVM_precict)\n#     return SVM_precict",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Code below is used for total pipeline, that integrated all levels and paraphrasing module",
   "metadata": {
    "cell_id": "f698da80d243467fbbc7ede0599ab48f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7fdbc80aa15b4ff6a33315d8c3f993e5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6d1d82d0",
    "execution_start": 1656150892860,
    "execution_millis": 5816,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1823.5
   },
   "source": "import os\nimport re\nimport math\nimport torch\nimport json\nimport pickle\nimport requests\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n\nimport itertools\nfrom operator import itemgetter\nfrom transformers import *\nfrom transformers import pipeline\nfrom ludwig.api import LudwigModel\nfrom transformers import BertTokenizer\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nprint(\"Num_threads:\", tf.config.threading.get_intra_op_parallelism_threads())  # to sort conflict with Ludwig\nprint(\"Num_threads:\", tf.config.threading.get_inter_op_parallelism_threads())  # to sort conflict with Ludwig\n\ntf.config.threading.set_inter_op_parallelism_threads(1)\ntf.config.threading.set_intra_op_parallelism_threads(1)  # to sort conflict with Ludwig\n\nnum_features=5   # Lime attributes\nlabel_names = [0,1]  # The labels whose explanation is needed\nLIME_num_samples=500    # The number of training instances LIME takes to create the perturbation\nfrom lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=label_names)\n\nconfig={\"L1_RO_model\": \"bert-base-uncased\",\n        \"L2_RO_model\": \"bert-base-cased\",\n        \"L1_num_labels\": 1,\n        \"L2_num_labels\": 3,\n        \"batch_size\": 16,\n        \"add_special_tokens\": True,\n        \"max_length\": 250,\n        \"padding\": True,\n        \"truncation\": True,\n        \"return_attention_mask\": True,\n        \"random_seed\": 42}   \n\nprediction_threshold = 0.5\nl1_RO_model=None\nl1_RO_tokenizer=None\nl2_RO_model=None\nl2_RO_tokenizer=None\nl2_UN_model=None\nl2_UN_vectorizer=None\nRO_to_UN_roberta_model=None\n\n\nfrom transformers import logging as hf_logging\nhf_logging.set_verbosity_warning()\nfrom transformers import RobertaTokenizer, RobertaForMaskedLM\nroberta_tokenizer=RobertaTokenizer.from_pretrained('roberta-base')\nfluency_roberta_model=RobertaForMaskedLM.from_pretrained('roberta-base')\n\n\nfrom sentence_transformers import SentenceTransformer, util\nsimilarity_score_model = SentenceTransformer('nli-distilroberta-base-v2')\n\n\nfrom simpletransformers.seq2seq import Seq2SeqModel\nseq2seq_model_args = {\n        \"reprocess_input_data\": True,\n        \"overwrite_output_dir\": True,\n        \"max_seq_length\": 250,\n        \"num_beams\":None,\n        \"train_batch_size\": 8,\n        \"num_train_epochs\": 2,\n        \"save_eval_checkpoints\": False,\n        \"save_model_every_epoch\": False,\n        \"evaluate_generated_text\": True,\n        \"learning_rate\" : 5e-5,\n        \"do_sample\":True,\n        \"top_k\":50,\n        \"top_p\":0.95,\n        \"num_return_sequences\":3,\n        \"evaluate_during_training\": False,\n        \"evaluate_during_training_verbose\": False,\n        \"use_multiprocessing\" :False,\n        \"reprocess_input_data\": True,\n    }\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "Num_threads: 1\nNum_threads: 1\nSome weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWARNING:root:You try to use a model that was created with version 1.1.0, however, your version is 0.3.8. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n\n\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3c827e70da8c4ea8ae5aa535c57a5750",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5dc078cc",
    "execution_start": 1656150905262,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1359
   },
   "source": "def Mask_Regex(Comment):\n    Regex_df=pd.read_csv(\"/datasets/ml-models-for-the-tool/Tool_Folder/Regex_Updated.csv\")\n    Regexes=Regex_df[\"Regex_Text\"]\n\n    for Regex in Regexes:\n        # print(Regex)\n        pattern = re.compile(Regex)\n        for m in pattern.finditer(Comment):\n            # print(m.start(),m.end(),m.group(),Regex,Regex_Data.loc[Regex_Data[\"Regex_Text\"]==Regex,\"Regex_Class\"].iloc[0])\n            location = {\"Start\": m.start(),\n                        \"End\": m.end()}\n            # print(location)\n            Sub_String=Comment[location['Start']:location['End']]\n            word=r\"[a-zA-Z0-9]*\"+re.escape(Sub_String)+r\"[a-zA-Z0-9]*\"\n            Comment=re.sub(word,'<mask>', Comment).strip()\n            # print(Comment)\n            break\n    return  Comment\n\ndef get_paraphrase_sugestions(sentence):\n    token_ids = roberta_tokenizer.encode(sentence, return_tensors='pt')\n    masked_position = torch.nonzero(token_ids.squeeze() == roberta_tokenizer.mask_token_id)\n    masked_pos = [mask.item() for mask in masked_position]\n\n    output = RO_to_UN_roberta_model(token_ids)\n    last_hidden_state = output[0].squeeze()\n    list_of_suggestions = []\n    for index, mask_index in enumerate(masked_pos):\n        mask_hidden_state = last_hidden_state[mask_index]\n        idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n        words = [roberta_tokenizer.decode(i.item()).strip() for i in idx]\n        list_of_suggestions.append(words)        \n    # print(list_of_suggestions)\n    return list_of_suggestions\n\ndef remove_regex(suggestions):\n    censored_suggestions =[]\n    Regex_df = pd.read_csv(\"/datasets/ml-models-for-the-tool/Tool_Folder/Regex_Updated.csv\")\n    Regexes = Regex_df[\"Regex_Text\"]\n    for suggestion in suggestions:\n        comment=\" \".join(suggestion)\n        # print(comment)\n        for Regex in Regexes:\n            # print(comment)\n            pattern = re.compile(Regex)\n            for m in pattern.finditer(comment):\n                location = {\"Start\": m.start(),\n                            \"End\": m.end()}\n                # print(location)\n                Sub_String = comment[location['Start']:location['End']]\n                # print(Sub_String)\n                word=r\"[a-zA-Z0-9]*\"+re.escape(Sub_String)+r\"[a-zA-Z0-9]*\"\n                comment = re.sub(word, '', comment).strip()\n                # print(comment)\n                break\n        # print(comment)\n        comment=re.sub(' +', ' ', comment)      # remove multiple spaces, otherwise that will be an entry in the list\n        censored_words=comment.split(\" \")\n        censored_suggestions.append(censored_words)\n    return censored_suggestions\n\ndef infill_suggestions(comment, censored_suggestions): # function to fill the masks with the suggestions\n    comment_restore = comment\n    filled_suggestions = []\n    for option in itertools.product(*censored_suggestions):\n        comment = comment_restore\n        for token in option:\n            comment = comment.replace('<mask>', token,\n                                      1)  # 1 -> replace only once, ie, the first instance from left to right\n        filled_suggestions.append(comment)\n    # print(\"Filled in suggestions: \", filled_suggestions)\n    return filled_suggestions",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "fde69a18bc7f401790d5d7b17275998b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58fe1492",
    "execution_start": 1656150908213,
    "execution_millis": 4,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 819
   },
   "source": "def classification_accuracy(suggestions):\n    classifier = pipeline(\"sentiment-analysis\")   # this is the bert model trained on sst-2 (GLUE dataset)\n    acc=[1 if ((classifier(suggestion)[0])['label']=='NEGATIVE') else 0 for suggestion in suggestions]\n    return np.array(acc)\n\n\ndef semantic_similarity_calcularor(suggestions, actual_comment):\n    # Compute embedding for the suggestions and actual comment, then returns the cosine similarity distnace between those\n    embeddings1 = similarity_score_model.encode(suggestions, convert_to_tensor=True)\n    embeddings2 = similarity_score_model.encode(actual_comment, convert_to_tensor=True)\n    cosine_similarity = util.pytorch_cos_sim(embeddings1, embeddings2)  # Compute cosine-similarities\n    return cosine_similarity.numpy()[:,0]\n\n\ndef fluency_calculator(suggestions):    \n    fluency_score=[]\n    for suggestion in suggestions:\n        tokenize_input = roberta_tokenizer.tokenize(suggestion)\n        tensor_input = torch.tensor([roberta_tokenizer.convert_tokens_to_ids(tokenize_input)])\n        predictions = fluency_roberta_model(tensor_input)\n        predictions = torch.stack(list(predictions), dim=0)\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(predictions.squeeze(), tensor_input.squeeze()).data\n        fluency_score.append(math.exp(loss))\n    fluency_score=np.array(fluency_score, dtype=np.float32).reshape(len(fluency_score),1)\n    # print(\"fluency score:\" ,fluency_score)\n\n    scaler = MinMaxScaler(feature_range=(0.1, 1))  # default scale range=(0,1), this is to accommodate the minimum value as well.\n    fluency_normalized=scaler.fit_transform(fluency_score)\n    # print(\"fluency normalized:\", fluency_normalized)\n    return np.array(fluency_normalized).reshape(len(fluency_normalized),)\n\ndef calculate_j_score(suggestions, actual_comment):\n    accuracy=classification_accuracy(suggestions)\n    # print(\"sentiment accuracy:\", accuracy)\n    similarity_score=semantic_similarity_calcularor(suggestions, actual_comment)\n    # print(\"similarity score:\", similarity_score)\n    fluency_score=fluency_calculator(suggestions)\n    # print(\"fluency score:\", fluency_score)\n    return np.multiply(accuracy, similarity_score, fluency_score)\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c7641a640b7741e1ae27640570740e67",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5ec6e35",
    "execution_start": 1656150912105,
    "execution_millis": 6,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 927
   },
   "source": "def L1_BERT_predict(comments):\n    # print(comments)\n    bert_tokenizer=l1_RO_tokenizer\n    L1_RO_trained_model=l1_RO_model\n    test_input = bert_tokenizer.batch_encode_plus(comments,\n                                                  add_special_tokens=True,\n                                                  max_length=250,\n                                                  padding=True,\n                                                  truncation=True,\n                                                  return_attention_mask=True)\n\n    val_inp = np.asarray(test_input['input_ids'])\n    val_mask = np.asarray(test_input['attention_mask'])\n    logits = np.array(L1_RO_trained_model.predict([val_inp, val_mask], batch_size=16)).squeeze(axis=0)\n    # print(logits)\n    pred_prob = tf.nn.sigmoid(logits).numpy()  # adding a sigmoid layer\n    pred_prob = np.pad(pred_prob, ((0, 0), (1, 0)))  # Sigmoid returns only one probability, to make it 2, increasing the size of columns\n    pred_prob[:, 0] = 1 - pred_prob[:, 1]\n    # print(pred_prob)\n    return (pred_prob)\n\ndef Lime_BERT_explanation(comment):\n    # This module finds the explanation of a comment being offensive.\n    # Then it normalizes the weights of words using min-max scaling and returns the corresponding words.\n    hot_words=[]\n    bert_exp = explainer.explain_instance(str(comment[0]),\n                                          L1_BERT_predict,\n                                          num_features=num_features,\n                                          num_samples=LIME_num_samples,\n                                          labels=(1,)\n                                          )\n    # print (bert_exp.as_map())\n    # print(type(bert_exp))\n    explanation_raw= dict(sorted(bert_exp.as_list(), key=lambda x: x[1], reverse=True))\n    # print(\"Lime explanation :\", explanation_raw)\n\n    scaler = MinMaxScaler()    # min-max scaler to normalize Lime explanation values, default scale (0,1)\n    explanation_normalized=scaler.fit_transform(np.array(list(explanation_raw.values())).reshape(len(explanation_raw.values()),1))\n    explanation_normalized=explanation_normalized.reshape(len(explanation_normalized),).tolist()\n    # print(\"Lime normalized scores:\", explanation_normalized)\n\n    explanation_list = list(explanation_raw.items())\n\n    for score in explanation_normalized:    # Locate the words to be masked\n        if score >= 0.5:\n            index = explanation_normalized.index(score)\n            hot_words.append(explanation_list[index][0])\n    return hot_words",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cfd7bd6a77874064af825b1aa3025fa1",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4fbd4e15",
    "execution_start": 1656150916064,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 243
   },
   "source": "def seq2seq_paraphraser(comment):\n    model=Seq2SeqModel(\n        encoder_decoder_type=\"bart\",\n        encoder_decoder_name=\"/datasets/ml-models-for-the-tool/Tool_Folder/outputs\",     # Already trained model\n        args=seq2seq_model_args,\n        use_cuda=False,\n    )\n    # comment=comment_Preprocess(Comment)\n    return (model.predict([comment]))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a3cbcbb7a0864585bf19ac22d5a45590",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f490e1ee",
    "execution_start": 1656150918120,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 945
   },
   "source": "def paraphrase(comment):    \n    # load_models() lets cccccccc\n    paraphrase_model_status=0\n    actual_comment=comment\n\n    L1_Lime_words_to_mask=Lime_BERT_explanation([comment])\n    # print(\"Lime words to mask:\", L1_Lime_words_to_mask)\n\n    for word in L1_Lime_words_to_mask:      # LIME masking\n        comment=re.sub(r'\\b'+word+r'\\b','<mask>', comment)\n    # print(\"Comment with Lime masks:\", comment)\n\n    comment=Mask_Regex(comment)             # Regex masking\n    # print(\"Comment with Regex masks:\", comment)\n\n    total_suggestions=get_paraphrase_sugestions(comment)    # get the paraphrasing suggestions\n    # print(\"Total suggestions: \", total_suggestions)\n\n    censored_suggestions=remove_regex(total_suggestions)    # censor potential RO terms from the suggestions\n    # print(len(censored_suggestions))\n    if (len(censored_suggestions)==1 and censored_suggestions[0][0]==''):\n        censored_suggestions[0][0]=\"***\"\n    # print(\"Censored suggestions: \", censored_suggestions)\n\n    filled_suggestions=infill_suggestions(comment, censored_suggestions)\n    # print(\"filled in suggestions:\", filled_suggestions)\n\n    j_scores=calculate_j_score(filled_suggestions, actual_comment)\n    \n    # for suggestion, score in zip(filled_suggestions, j_scores):         # suggestions with J-scores\n    #     print(suggestion,\":\", score)\n    \n    final_suggestions=[(filled_suggestions[i],j_scores[i]) for i in range(len(j_scores)) if j_scores[i]>0.5]     # Ranking based on j_score    \n    if final_suggestions:        \n        # print(final_suggestions)\n        final_suggestions.sort(key=itemgetter(1),reverse=True)\n        # print(final_suggestions)\n        final_suggestions=[element[0] for element in final_suggestions]    #Take the suggestion only, not the scores\n        if len(final_suggestions)<=3:    # top 3 suggestions            \n            # print(final_suggestions)\n            return (paraphrase_model_status, final_suggestions)\n        else:\n            # print(final_suggestions[:5])\n            return (paraphrase_model_status, final_suggestions[:3])\n    else:                   # if no more suggestions from paraphrases, then use seq2seq outputs\n        # print(\"Suggestions from seq2seq model:\", seq2seq_paraphraser(actual_comment))          # Uncommnt these two lines for seq2seq model use\n        paraphrase_model_status=1\n        print(paraphrase_model_status)\n        return (paraphrase_model_status, seq2seq_paraphraser(actual_comment))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5d82ff705ff44986a5b73bac31b41f29",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d6c19309",
    "execution_start": 1656150925044,
    "execution_millis": 4,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 495
   },
   "source": "def load_models():      # to keep all the models loaded once into memory to avoid multiple fetching from memory\n    global l1_RO_model\n    global l1_RO_tokenizer\n    global l2_RO_model\n    global l2_RO_tokenizer\n    global l2_UN_model\n    global l2_UN_vectorizer\n    global l3_model    \n    global RO_to_UN_roberta_model\n    \n    l1_RO_tokenizer = BertTokenizer.from_pretrained(config[\"L1_RO_model\"])\n    l1_RO_model = TFBertForSequenceClassification.from_pretrained(config[\"L1_RO_model\"],num_labels=config[\"L1_num_labels\"])\n    l1_RO_model.load_weights('/datasets/ml-models-for-the-tool/Tool_Folder/L1_Transformer_BERT_Model.h5')\n\n\n    l2_RO_tokenizer = BertTokenizer.from_pretrained(config[\"L2_RO_model\"])\n    l2_RO_model = TFBertForSequenceClassification.from_pretrained(config[\"L2_RO_model\"],num_labels=config[\"L2_num_labels\"])\n    l2_RO_model.load_weights('/datasets/ml-models-for-the-tool/Tool_Folder/L2_Transformer_BERT_Model.h5')\n\n    # l2_UN_vectorizer=pickle.load(open(\"C:\\\\StackOverflow_Work\\\\ML models\\\\L2_UN_ML_models\\\\L2_UN_Vectorizer.pkl\", 'rb'))\n    # l2_UN_model=pickle.load(open(\"C:\\\\StackOverflow_Work\\\\ML models\\\\L2_UN_ML_models\\\\L2_UN_SVM_Classifier.pkl\"\n\n    RO_to_UN_roberta_model=RobertaForMaskedLM.from_pretrained(\"//datasets/ml-models-for-the-tool/Tool_Folder/RO_to_UN_roberta_model\")\n    ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "code below is used for L1, L2 and L3 models. No issues.",
   "metadata": {
    "cell_id": "0d8a1bbf33184975b848a7f092770d7a",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 52.390625
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "321dc2fc-7e48-41de-bc64-992b450532c4",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "66b5ac65",
    "execution_start": 1656150928539,
    "execution_millis": 4,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 621
   },
   "source": "def regex_check(comment):\n    regex_presence = 0\n    regex_df = pd.read_csv('/datasets/ml-models-for-the-tool/Tool_Folder/Regex_Updated.csv')\n    regexes = regex_df[\"Regex_Text\"]\n    for regex in regexes:\n        pattern = re.compile(regex)\n        if pattern.search(comment):\n            print(pattern)\n            regex_presence = 1\n            break\n    # print(\"Regex status: \", regex_presence)\n    return regex_presence\n\ndef PAPI_check(comment):\n    papi_api_key = 'AIzaSyAAhtjSbNwJUFh32JAxtiydqiye87kyfqM'\n    PAPI_url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' + '?key=' + papi_api_key)\n    data_dict = {'comment':\n                {'text': comment},\n                 'languages': ['en'],\n                 'doNotStore': True,\n                 'requestedAttributes': {'TOXICITY': {}}\n                 }\n    response = requests.post(url=PAPI_url, data=json.dumps(data_dict))\n    response_dict = json.loads(response.content)\n    PAPI_score = response_dict[\"attributeScores\"][\"TOXICITY\"]['summaryScore']['value']\n    # print(\"PAPI score: \", PAPI_score)\n    return PAPI_score\n\ndef regex_and_PAPI_check(comment):      # this function returns TRue if both Regex is prsent and PAPI score>=0.7, else False\n    return 1 if(regex_check(comment) and PAPI_check(comment)>=0.7) else 0\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3e9e2fda-f574-4d33-a727-f498acc0ea23",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "94bba08c",
    "execution_start": 1656150930957,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1377
   },
   "source": "def level_3_prediction(comment):    \n    print(\"Num_threads:\", tf.config.threading.get_intra_op_parallelism_threads())  # to sort conflict with Ludwig\n    print(\"Num_threads:\", tf.config.threading.get_inter_op_parallelism_threads())  # to sort conflict with Ludwig\n    ludwig_model=LudwigModel.load(\"/datasets/ml-models-for-the-tool/Tool_Folder/L3_Model\", \n                                allow_parallel_threads=False\n                                        )  # Already trained model\n\n    comment = {\"Comment_Text\": comment, \"Racial\": '', \"Swearing\": ''}\n    Df = pd.DataFrame(columns=[\"Comment_Text\", \"Racial\", \"Swearing\"])\n    Df = Df.append(comment, ignore_index=True)\n    predictions, _ = ludwig_model.predict(dataset=Df)\n    # print(\"Ludwig predictions\", predictions)\n\n    return (predictions['Racial_predictions'][0],\n            predictions['Swearing_predictions'][0])\n\n\ndef level_2_RO_TASA_prediction(comment):  # this function accepts the comment and gets tf-idf features\n    \n    # this block loads the vectorizer, and converts the comment to tf-idf vector\n    vectorizer = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/L2_RO_TASA_TF_IDF_Vectorizer.pkl', 'rb'))\n    vect = vectorizer.transform([comment])\n    vect = pd.DataFrame(vect.toarray(), columns=vectorizer.get_feature_names())    \n    \n    SVM = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/L2_RO_TASA_TF-IDF_SVM_Classifier.pkl', 'rb'))\n    SVM_precict = SVM.predict(vect)\n\n    # print(\"\\n Level 2 UN prediction :\", SVM_precict)\n    return SVM_precict\n           \n\n\ndef level_2_RO_prediction(comment): \n    bert_tokenizer=l2_RO_tokenizer\n    trained_model=l2_RO_model\n    test_input = bert_tokenizer.batch_encode_plus(comment,\n                                                  add_special_tokens=config[\"add_special_tokens\"],\n                                                  max_length=config[\"max_length\"],\n                                                  padding=config[\"padding\"],\n                                                  truncation=config[\"truncation\"],\n                                                  return_attention_mask=config[\"return_attention_mask\"]\n                                                    )\n\n    val_inp = np.asarray(test_input['input_ids'])\n    val_mask = np.asarray(test_input['attention_mask'])\n\n    # code  for 1 label prediction      Use sigmoid for binary prediction\n    logits = np.array(trained_model.predict([val_inp, val_mask])).squeeze(axis=0)  # code for single label prediction\n    pred_prob = tf.nn.sigmoid(logits).numpy()\n    pred_labels = np.where(pred_prob < prediction_threshold, 0, 1)  # for multi label prediction\n    # print(pred_labels)\n    return pred_labels[0]\n\ndef level_1_RO_BERT_prediction(comment):  \n    bert_tokenizer=l1_RO_tokenizer\n    trained_model =l1_RO_model\n    test_input = bert_tokenizer.batch_encode_plus(comment,                                                \n                                                add_special_tokens=config[\"add_special_tokens\"],\n                                                  max_length=config[\"max_length\"],\n                                                  padding=config[\"padding\"],\n                                                  truncation=config[\"truncation\"],\n                                                  return_attention_mask=config[\"return_attention_mask\"]\n                                                )\n\n    val_inp = np.asarray(test_input['input_ids'])\n    val_mask = np.asarray(test_input['attention_mask'])\n    logits = np.array(trained_model.predict([val_inp, val_mask], batch_size=config[\"batch_size\"])).squeeze(axis=0)\n    pred_prob = tf.nn.sigmoid(logits).numpy()       # adding a sigmoid layer to logits to get pred_probabilities\n    # print(pred_prob)\n    # pred_label = 0 if pred_prob[0][0] <= prediction_threshold else 1\n    pred_label = np.where(pred_prob <= prediction_threshold, 0, 1)\n    # print(\"Level 1 RO prediction:\", pred_label[0][0])\n    return pred_label",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6c9476edb45b4377a57d90578c69a6e6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b24ff47b",
    "execution_start": 1656150935643,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 549
   },
   "source": "def level_2_UN_prediction(comment):  # this function accepts the comment, gets politeness and tf-idf features\n    \n    # this block loads the vectrizer, and converts the comment to tf-idf vector, then concatenates with the politeness features\n    vectorizer = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/UN_Temporary_models/L2_UN_Vectorizer.pkl', 'rb'))\n    vect = vectorizer.transform([comment])\n    vect = pd.DataFrame(vect.toarray(), columns=vectorizer.get_feature_names())    \n    \n    SVM = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/UN_Temporary_models/L2_UN_SVM_Classifier.pkl', 'rb'))\n    SVM_precict = SVM.predict(vect)\n\n    # print(\"\\n Level 2 UN prediction :\", SVM_precict)\n    return SVM_precict\n\n\ndef level_1_UN_prediction(comment):  # this function accepts the comment, gets politeness, sentimental and tf-idf features\n    # df_polite = get_politeness_features(comment)\n\n    # this block loads the vectrizer, and converts the comment to tf-idf vector, then concatenates with the politeness features\n    vectorizer = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/UN_Temporary_models/L1_UN_Vectorizer.pkl', 'rb'))\n    vect = vectorizer.transform([comment])\n    vect = pd.DataFrame(vect.toarray(), columns=vectorizer.get_feature_names())    \n    \n    SVM = pickle.load(open('/datasets/ml-models-for-the-tool/Tool_Folder/UN_Temporary_models/L1_UN_SVM_Classifier.pkl', 'rb'))\n    SVM_precict = SVM.predict(vect)\n\n    # print(\"\\n Level 1 UN prediction :\", SVM_precict)\n    return SVM_precict",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5368cf48-a5db-40e4-b21e-a42c9da3c471",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "67d5b789",
    "execution_start": 1656150940223,
    "execution_millis": 84736,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2413.8125
   },
   "source": "def analyze(comment):    \n    Level_1_RO=0\n    Level_1_UN=0\n    if (level_1_RO_BERT_prediction([comment])):     # RO pipeline        \n        if(regex_and_PAPI_check(comment)):      # if yes, go to Levels 2 and 3, then paraphrase\n            Level_1_RO = 1\n            # print(\"Level 1 RO = 1\")\n            \n            l2_RO_outcomes = level_2_RO_prediction(comment)\n            # print(\"Individually targetted: \", l2_RO_outcomes[0])\n            # print(\"Group targetted: \", l2_RO_outcomes[1])\n            # print(\"Others: \", l2_RO_outcomes[2])\n            l2_RO_TASA_outcomes=level_2_RO_TASA_prediction(comment)\n\n            l3_outcomes = level_3_prediction(comment)\n            # print(\"Racial abuse:\", l3_outcomes[0])\n            # print(\"Swearing abuse:\", l3_outcomes[1])            \n                        \n            # L1_RO,L1_UN, L2_individual, L2_Group, L2_TA, L2_SA, L2_Others, L3_Racial, L3_Swearing\n            return (Level_1_RO, Level_1_UN, l2_RO_outcomes[0], \n            l2_RO_outcomes[1], l2_RO_TASA_outcomes[0][0], l2_RO_TASA_outcomes[0][1], l2_RO_outcomes[2], \n            l3_outcomes[0], l3_outcomes[1] )                 \n    \n\n        else:    # no regex no PAPI, but may be UN comment, so check\n            # print(\"Level 1 RO=0, UN checking\")            \n            if(level_1_UN_prediction(comment)):\n                # print(\"Level 1 UN =1\")\n                Level_1_UN=1                \n                l2_UN_outcomes = level_2_UN_prediction(comment)\n                # print(\"Individually targetted: \", l2_UN_outcomes[0][0])\n                # print(\"Group targetted: \", l2_UN_outcomes[0][1])\n                # print(\"Entity abuse: \", l2_UN_outcomes[0][2])\n                # print(\"Others: \", l2_UN_outcomes[0][3])\n\n                # L1_RO,L1_UN, L2_individual, L2_Group, L2_TA, L2_SA, L2_Others, L3_Racial, L3_Swearing\n                return (Level_1_RO, Level_1_UN, l2_UN_outcomes[0][0], l2_UN_outcomes[0][1], l2_UN_outcomes[0][2], 0, l2_UN_outcomes[0][3], 0, 0)                 \n            else:\n                # print(\"Level 1 UN=0\")\n                # print(\"No violations\")\n                # L1_RO,L1_UN, L2_individual, L2_Group, L2_TA, L2_SA, L2_Others, L3_Racial, L3_Swearing\n                return (0, 0, 0, 0, 0, 0, 0, 0, 0 )                 \n    else:\n        # print(\"Level 1 RO=0, UN checking\")            \n        if(level_1_UN_prediction(comment)):\n            # print(\"Level 1 UN=1\")\n            Level_1_UN=1\n            l2_UN_outcomes = level_2_UN_prediction(comment)\n            # print(\"Individually targetted: \", l2_UN_outcomes[0][0])\n            # print(\"Group targetted: \", l2_UN_outcomes[0][1])\n            # print(\"Entity abuse: \", l2_UN_outcomes[0][2])\n            # print(\"Others: \", l2_UN_outcomes[0][3])\n\n            # L1_RO,L1_UN, L2_individual, L2_Group, L2_TA, L2_SA, L2_Others, L3_Racial, L3_Swearing\n            return (Level_1_RO, Level_1_UN, l2_UN_outcomes[0][0], l2_UN_outcomes[0][1], l2_UN_outcomes[0][2], 0, l2_UN_outcomes[0][3], 0, 0)                 \n        else:\n            # print(\"Level 1 Un=0\")\n            # print(\"No violations\")     \n            # L1_RO,L1_UN, L2_individual, L2_Group, L2_TA, L2_SA, L2_Others, L3_Racial, L3_Swearing\n            return (0, 0, 0, 0, 0, 0, 0, 0, 0 )                 \n\n\nif __name__=='__main__':\n    comment=\"people like you write fucking comments which is not software related\"\n    load_models()\n    analyze(comment)\n",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0fcd77e3a074431ab90ccafc33428c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f5e8038f96e411f9df9f7cfebb87f85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9313cc65dd9145d4b7b48743326479cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b6b2a99666642f3b79a7075f251625a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33a5665fa43544dabdc78ba78a3b9656"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/527M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e508db1a7754caf907207170de6035b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-cased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['dropout_75', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nre.compile('(?i)\\\\bf(.*)ck(s|ers|ed)?', re.IGNORECASE)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\nNum_threads: 1\nNum_threads: 1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "03bbc80e-9604-4ef4-a05b-29e1e0af3f57",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a9ebe654",
    "execution_start": 1656151034362,
    "execution_millis": 523,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 188.5625
   },
   "source": "\nimport anvil.server\nanvil.server.connect(\"M7JODHX7ATALW6DFMUYMIHGU-DBZHCQVHA2BPGTHT\")",
   "outputs": [
    {
     "name": "stdout",
     "text": "Connecting to wss://anvil.works/uplink\nAnvil websocket open\nConnected to \"Default environment\" as SERVER\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1bc40e7f-2220-4803-890a-ca1b5e45576f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "25823d30",
    "execution_start": 1656151037465,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "source": "@anvil.server.callable\ndef classify_text(comment):       \n\n    return analyze(comment)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6d35ade8974d4f5b8266f5c1b92ea623",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6bef037d",
    "execution_start": 1656151041059,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135
   },
   "source": "@anvil.server.callable\ndef paraprasing(comment):       \n\n    return paraphrase(comment)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "28691835-48c7-4568-aab7-3909d081c026",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7ae2c9df",
    "execution_start": 1656151044448,
    "execution_millis": 752083,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1890,
    "deepnote_output_heights": [
     null,
     null,
     null,
     null,
     null,
     null,
     213.0625
    ]
   },
   "source": "anvil.server.wait_forever()",
   "outputs": [
    {
     "name": "stdout",
     "text": "re.compile('(?i)\\\\bf(.*)ck(s|ers|ed)?', re.IGNORECASE)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\nNum_threads: 1\nNum_threads: 1\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b18abc30bd534ecea613e9cbbfaf7f7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d75c67716f9495e99e1002780713bf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/230 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96135d6d91b844be975164ad0caedc89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cba321c637d4e92bd87a539c5bff179"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": "re.compile('(?i)\\\\bf(.*)ck(s|ers|ed)?', re.IGNORECASE)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\nNum_threads: 1\nNum_threads: 1\nre.compile('(?i)(\\\\b)silly', re.IGNORECASE)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator SVC from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n  UserWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py:444: UserWarning: X has feature names, but SVC was fitted without feature names\n  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d26ace4-b35e-41ca-becb-b00c6d134efb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "44079079-212b-40d1-a02e-78ad9af02076",
  "deepnote_execution_queue": []
 }
}